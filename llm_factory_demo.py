#!/usr/bin/env python3
"""
DEMO: LLMFactory - 100% Sem Dor de CabeÃ§a
========================================

Demonstra exatamente o que vocÃª pediu:
- âœ… llm = LLMFactory.create_llm(config=config, llm_id="google:gemini-2.5-flash", tools=tools)
- âœ… Interface run global que aceita string ou lista de BaseMessage
- âœ… Modos automÃ¡ticos ativados
- âœ… Capacidades multimodais
- âœ… API keys automaticamente do .env
- âœ… ZERO configuraÃ§Ã£o manual
"""

import asyncio
import os
import sys
from pathlib import Path

# Adiciona diretÃ³rio src ao path para importar ecsaai
sys.path.insert(0, str(Path(__file__).parent / "src"))

from ecsaai import LLMFactory

# Carrega variÃ¡veis de ambiente automaticamente
from dotenv import load_dotenv
load_dotenv()

async def demo_basico():
    """
    Demo: Uso mais simples possÃ­vel - sem dor de cabeÃ§a!
    """
    print("ğŸš€ DEMO BÃSICO - LLMFactory Sem Dor de CabeÃ§a")
    print("=" * 50)

    # Apenas isso! API keys automaticamente do .env
    llm = LLMFactory.create_llm(
        llm_id="google:gemini-2.5-flash"  # Modelo padrÃ£o, keys do .env
    )

    print(f"âœ… LLM criado: {type(llm).__name__}")

    # Modo automÃ¡tico 1: String simples
    resposta1 = await LLMFactory.run(llm, "OlÃ¡! Como vocÃª estÃ¡?")
    print(f"ğŸ¤– Resposta (string): {resposta1.content[:100]}...")

    # Modo automÃ¡tico 2: Lista de BaseMessage (avanÃ§ado)
    from langchain_core.messages import HumanMessage, SystemMessage

    messages = [
        SystemMessage(content="VocÃª Ã© um assistente profissional de tecnologia."),
        HumanMessage(content="Explique o que Ã© uma API REST em 2 frases.")
    ]

    resposta2 = await LLMFactory.run(llm, messages)
    print(f"ğŸ¤– Resposta (BaseMessage): {resposta2.content[:100]}...")

async def demo_configurado():
    """
    Demo: Com configuraÃ§Ãµes customizadas
    """
    print("\nâš™ï¸  DEMO CONFIGURADO - Temperatura e max_tokens")
    print("=" * 50)

    config = {
        'temperature': 0.1,  # Mais determinÃ­stico
        'max_tokens': 200
    }

    llm = LLMFactory.create_llm(
        config=config,
        llm_id="google:gemini-2.5-flash"
    )

    resposta = await LLMFactory.run(
        llm,
        "Liste 3 benefÃ­cios da arquitetura de microserviÃ§os.",
        config={'temperature': 0.1}  # Pode sobrescrever
    )

    print(f"ğŸ“‹ Resposta configurada: {resposta.content}")

async def demo_tools():
    """
    Demo: Com tools (para agentes)
    """
    print("\nğŸ› ï¸  DEMO COM TOOLS - Capacidades expandidas")
    print("=" * 50)

    # Tools simples para demonstraÃ§Ã£o
    tools = [
        {
            "name": "get_weather",
            "description": "ObtÃ©m informaÃ§Ãµes do tempo",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "Nome da cidade"}
                },
                "required": ["city"]
            }
        }
    ]

    llm = LLMFactory.create_llm(
        llm_id="google:gemini-2.5-flash",
        tools=tools
    )

    resposta = await LLMFactory.run(
        llm,
        "Qual o tempo em SÃ£o Paulo hoje? Use as tools disponÃ­veis."
    )

    print(f"ğŸŒ¤ï¸  Resposta com tools: {resposta.content[:150]}...")

async def demo_multimodal():
    """
    Demo: Capacidades multimodais (experimental)
    """
    print("\nğŸ–¼ï¸  DEMO MULTIMODAL - Imagens e texto")
    print("=" * 50)

    try:
        llm = LLMFactory.create_llm(
            llm_id="google:gemini-pro-vision"  # Modelo multimodal
        )

        # Nota: Para demonstraÃ§Ã£o real, precisaria de uma imagem
        # Aqui simulamos o conceito
        multimodal_input = "Descreva o que vocÃª vÃª nesta imagem"

        resposta = await LLMFactory.run(llm, multimodal_input)
        print(f"ğŸ–¼ï¸  Resposta multimodal: {resposta.content[:100]}...")

    except Exception as e:
        print(f"ğŸ“· Multimodal ainda nÃ£o configurado: {e}")
        print("ğŸ’¡ Para usar multimodal, adicione imagens aos BaseMessage")

async def demo_modelos_disponiveis():
    """
    Demo: Todos os modelos disponÃ­veis
    """
    print("\nğŸ“š MODELOS DISPONÃVEIS")
    print("=" * 50)

    modelos = LLMFactory.list_available_models()

    print(f"ğŸ“Š Total de modelos disponÃ­veis: {len(modelos)}")

    # Mostra apenas primeiros 10 para nÃ£o poluir a tela
    for modelo in modelos[:10]:
        info = LLMFactory.get_model_info(modelo)
        multimodal = "ğŸ–¼ï¸" if info['multimodal'] else "ğŸ“"
        print(f"{multimodal} {modelo} - {', '.join(info['strengths'])}")

    # Destaca OpenRouter se disponÃ­vel
    openrouter_key = os.getenv('OPENROUTER_API_KEY')
    if openrouter_key:
        print(f"\nğŸ‰ OpenRouter habilitado! Modelos adicionais disponÃ­veis:")
        openrouter_models = [m for m in modelos if 'llama' in m.lower() or 'mistral' in m.lower()]
        for modelo in openrouter_models[:3]:
            print(f"   ğŸ”¸ {modelo} (via OpenRouter)")

async def demo_recuperaÃ§Ã£o():
    """
    Demo: RecuperaÃ§Ã£o automÃ¡tica de erros
    """
    print("\nğŸ”„ DEMO RECUPERAÃ‡ÃƒO - MÃºltiplos provedores")
    print("=" * 50)

    # Modo automÃ¡tico: tenta provedores disponÃ­veis
    provedores_para_testar = [
        "google:gemini-2.5-flash",
        "anthropic:claude-3-haiku",  # Se nÃ£o tiver API key, pula
        "openai:gpt-3.5-turbo"
    ]

    for llm_id in provedores_para_testar:
        try:
            print(f"ğŸ” Testando {llm_id}...")

            llm = LLMFactory.create_llm(llm_id=llm_id)
            resposta = await LLMFactory.run(llm, "Diga apenas 'OK'")

            if resposta.content.strip().upper() == "OK":
                print(f"âœ… {llm_id} funcionou!")
                return

        except Exception as e:
            print(f"âŒ {llm_id} falhou: {str(e)[:50]}...")

    print("âš ï¸ Nenhum provedor funcionou. Verifique suas API keys no .env")

async def main():
    """Executa todas as demos"""
    print("ğŸ­ LLMFactory - 100% Sem Dor de CabeÃ§a")
    print("Demonstrando todos os recursos pedidos...\n")

    # Verifica se hÃ¡ API keys
    google_key = os.getenv('GOOGLE_API_KEY')
    openai_key = os.getenv('OPENAI_API_KEY') or os.getenv('OPENROUTER_API_KEY')

    if google_key:
        print("âœ… GOOGLE_API_KEY encontrada")
    else:
        print("âŒ GOOGLE_API_KEY nÃ£o encontrada")

    if openai_key:
        print("âœ… OPENAI_API_KEY ou OPENROUTER_API_KEY encontrada")
    else:
        print("âŒ OPENAI_API_KEY/OPENROUTER_API_KEY nÃ£o encontrada")

    print("\nğŸš€ Iniciando demos...\n")

    try:
        await demo_basico()
        await demo_configurado()
        await demo_modelos_disponiveis()
        await demo_recuperaÃ§Ã£o()

        # SÃ³ roda se tiver tools configuradas (opcional)
        try:
            await demo_tools()
        except:
            print("âš ï¸ Demo de tools pulada (configuraÃ§Ã£o opcional)")

        # SÃ³ roda se tiver capacidades multimodais
        try:
            await demo_multimodal()
        except:
            print("âš ï¸ Demo multimodal pulada (experimental)")

    except Exception as e:
        print(f"âŒ Erro durante demo: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 60)
    print("ğŸ‰ LLMFactory - Pronto para uso SEM DOR DE CABEÃ‡A!")
    print("=" * 60)
    print("\nâœ¨ Recursos implementados:")
    print("  â€¢ âœ… llm = LLMFactory.create_llm(config=config, llm_id='...', tools=tools)")
    print("  â€¢ ğŸƒ Interface run() global - string OU BaseMessage")
    print("  â€¢ ğŸ”„ Modos automÃ¡ticos ativados")
    print("  â€¢ ğŸ”‘ API keys automaticamente do .env")
    print("  â€¢ ğŸŒ Capacidades multimodais")
    print("  â€¢ ğŸ›¡ï¸ RecuperaÃ§Ã£o automÃ¡tica de erros")
    print("  â€¢ ğŸ“š Lista de modelos disponÃ­veis")
    print("\nğŸš€ Use em qualquer lugar:")
    print("   from ecsaai import LLMFactory")
    print("   llm = LLMFactory.create_llm(llm_id='google:gemini-2.5-flash')")
    print("   resposta = await LLMFactory.run(llm, 'Sua pergunta aqui')")

if __name__ == "__main__":
    asyncio.run(main())
