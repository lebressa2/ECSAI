# ğŸš€ ECSAI Framework - Guia Completo da API

**Framework ECSA (Event-Driven Component System Architecture) com LLMFactory sem dor de cabeÃ§a!**

---

## ğŸ“– Ãndice

- [ğŸš€ IntroduÃ§Ã£o](#-introduÃ§Ã£o)
- [âš¡ InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ğŸ”§ ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [ğŸ§  LLMFactory - Zero Dor de CabeÃ§a](#-llmfactory---zero-dor-de-cabeÃ§a)
- [ğŸ—ï¸ Arquitetura Event-Driven](#ï¸-arquitetura-event-driven)
- [ğŸ› ï¸ Sistema de Componentes](#ï¸-sistema-de-componentes)
- [ğŸ¯ Middleware System](#-middleware-system)
- [ğŸ­ Exemplos PrÃ¡ticos](#-exemplos-prÃ¡ticos)
- [ğŸ”§ Receitas AvanÃ§adas](#-receitas-avanÃ§adas)
- [ğŸ› Troubleshooting](#-troubleshooting)

---

## ğŸš€ IntroduÃ§Ã£o

O **ECSAI Framework** Ã© um framework moderno e poderoso para construÃ§Ã£o de aplicaÃ§Ãµes de IA usando arquitetura **event-driven** com **componentes modulares**. Ele elimina completamente a **dor de cabeÃ§a** de trabalhar com LLMs, oferecendo:

### âœ¨ **Por que ECSAI?**

- **ğŸ¯ Zero ConfiguraÃ§Ã£o**: API keys automaticamente do `.env`
- **ğŸ—ï¸ Arquitetura Moderna**: Event-driven com componentes desacoplados
- **ğŸ§  LLMFactory Poderoso**: Suporte a 20+ modelos (Google, OpenAI, Anthropic, Llama, Mistral via OpenRouter)
- **âš¡ Performance**: Middlewares para caching, rate limiting, circuit breaker
- **ğŸ”§ ExtensÃ­vel**: FÃ¡cil criar componentes customizados
- **ğŸ›¡ï¸ Robusto**: Tratamento automÃ¡tico de erros e fallbacks

### ğŸ—ï¸ **Arquitetura Core**

```python
# 1. Componentes comunicam via Events (nÃ£o diretamente)
ComponentA â†’ Event â†’ ComponentB â†’ Event â†’ ComponentC

# 2. LLMFactory cuida dos LLMs automaticamente
llm = LLMFactory.create_llm(llm_id="google:gemini-2.5-flash")
response = await LLMFactory.run(llm, "Sua pergunta")

# 3. Middlewares interceptam tudo automaticamente
LoggingMiddleware â†’ RateLimitMiddleware â†’ CircuitBreakerMiddleware
```

---

## âš¡ InstalaÃ§Ã£o

### ğŸ“¦ **Via pip (Recomendado)**

```bash
pip install ecsaai
```

### ğŸ› ï¸ **InstalaÃ§Ã£o Manual**

```bash
git clone https://github.com/your-repo/ecsaai.git
cd ecsaai
pip install -e .
```

### ğŸ” **Verificar InstalaÃ§Ã£o**

```python
import ecsaai
print(ecsaai.__version__)  # Deve mostrar versÃ£o atual
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### ğŸ“„ **Arquivo .env**

Crie um arquivo `.env` na raiz do seu projeto:

```env
# API Keys (automaticamente detectadas)
GOOGLE_API_KEY=your_google_api_key
OPENROUTER_API_KEY=sk-or-v1-...your_openrouter_key
OPENAI_API_KEY=sk-...your_openai_key  # Opcional, cai para OpenRouter se ausente
ANTHROPIC_API_KEY=sk-ant-...your_anthropic_key
COHERE_API_KEY=your_cohere_key
HUGGINGFACE_API_TOKEN=hf_...your_huggingface_token

# ConfiguraÃ§Ãµes opcionais
ECS_LOG_LEVEL=INFO
ECS_MAX_RETRIES=3
```

### ğŸ¯ **DetecÃ§Ã£o AutomÃ¡tica de Provedores**

O framework **prioriza provedores automaticamente**:

1. **OpenRouter** (se `OPENROUTER_API_KEY` presente) - Mais econÃ´mico
2. **Google Gemini** (sempre disponÃ­vel) - Gratuito atÃ© certo limite
3. **OpenAI direto** (se `OPENAI_API_KEY` presente)
4. **Anthropic** (se `ANTHROPIC_API_KEY` presente)
5. **Outros** (Cohere, HuggingFace)

---

## ğŸ§  LLMFactory - Zero Dor de CabeÃ§a

### ğŸƒ **Interface Simples**

```python
from ecsaai import LLMFactory

# Exemplo mais simples possÃ­vel
llm = LLMFactory.create_llm(llm_id="google:gemini-2.5-flash")
resposta = await LLMFactory.run(llm, "OlÃ¡, como vocÃª estÃ¡?")
print(resposta.content)
```

### ğŸ”§ **Com ConfiguraÃ§Ã£o**

```python
# Com configuraÃ§Ãµes customizadas
config = {
    'temperature': 0.1,     # Mais determinÃ­stico (0.0 = sempre igual, 1.0 = criativo)
    'max_tokens': 1000,     # MÃ¡ximo de tokens na resposta
    'timeout': 30,          # Timeout em segundos
}

llm = LLMFactory.create_llm(
    config=config,
    llm_id="google:gemini-2.5-flash",
    tools=tools_opcionais
)
```

### ğŸ“‹ **Modelos DisponÃ­veis**

```python
# Ver todos os modelos disponÃ­veis
modelos = LLMFactory.list_available_models()
print(f"ğŸ“Š {len(modelos)} modelos disponÃ­veis!")

# Modelos principais
llm = LLMFactory.create_llm(llm_id="google:gemini-2.5-flash")      # Mais rÃ¡pido
llm = LLMFactory.create_llm(llm_id="anthropic:claude-3-haiku")      # Mais inteligente
llm = LLMFactory.create_llm(llm_id="openai:gpt-4o-mini")            # Via OpenRouter (barato)
llm = LLMFactory.create_llm(llm_id="meta-llama:llama-3.1-70b-instruct")  # Open source
```

### ğŸƒ **Modos AutomÃ¡ticos da Interface run()**

```python
from ecsaai import LLMFactory

llm = LLMFactory.create_llm(llm_id="google:gemini-2.5-flash")

# MODO 1: String simples (automÃ¡tico)
resposta = await LLMFactory.run(llm, "Explique APIs REST")

# MODO 2: Lista de BaseMessage (avanÃ§ado)
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="VocÃª Ã© um professor paciente"),
    HumanMessage(content="Me explique sobre microserviÃ§os")
]

resposta = await LLMFactory.run(llm, messages)
```

### ğŸ› ï¸ **Com Tools (Agentes)**

```python
from ecsaai import LLMFactory

# Definir tools
tools = [
    {
        "name": "get_weather",
        "description": "ObtÃ©m informaÃ§Ãµes do clima",
        "parameters": {
            "type": "object",
            "properties": {"city": {"type": "string"}},
            "required": ["city"]
        }
    }
]

llm = LLMFactory.create_llm(
    llm_id="google:gemini-2.5-flash",
    tools=tools
)

resposta = await LLMFactory.run(llm, "Qual o clima em SÃ£o Paulo?")
# LLM automaticamente chama tools quando necessÃ¡rio!
```

### ğŸŒ **Capacidades Multimodais**

```python
from ecsaai import LLMFactory
from langchain_core.messages import HumanMessage

# Para imagens
llm = LLMFactory.create_llm(llm_id="google:gemini-pro-vision")

messages = [
    HumanMessage(content=[
        {"type": "text", "text": "O que vocÃª vÃª nesta imagem?"},
        {"type": "image_url", "image_url": "https://exemplo.com/imagem.jpg"}
    ])
]

resposta = await LLMFactory.run(llm, messages)
```

---

## ğŸ—ï¸ Arquitetura Event-Driven

### ğŸ¯ **O que Ã© Event-Driven?**

Imagine componentes como **caixas postais**: eles mandam mensagens (events) para outros componentes lerem, sem conhecer uns aos outros diretamente.

```
Componente A â†’ Event "dados_processados" â†’ Componente B â†’ Event "resposta_pronta" â†’ Output
```

### ğŸ—ï¸ **Estrutura BÃ¡sica**

```python
from ecsaai import Agent, Component, EventBus, BaseEvent

# 1. Criar Agent (contÃªiner de componentes)
agent = Agent("meu_agent")

# 2. Adicionar componentes
await agent.add_component(MeuComponent, config=minha_config)

# 3. Inicializar tudo
await agent.init_all()

# 4. Mandar primeiro evento
evento = InputEvent(content="OlÃ¡ mundo", session_id="sessao_1")
resultados = await agent.send_event(evento)
```

### ğŸ“¬ **Sistema de Eventos Built-in**

```python
from ecsaai.Events import (
    InputEvent,     # Entrada do usuÃ¡rio
    ContextEvent,   # Contexto formatado
    LLMResponseEvent, # Resposta do LLM
    OutputEvent,    # SaÃ­da final
    ErrorEvent      # Erro ocorrido
)

# Criar evento customizado
class MeuEvento(BaseEvent):
    type: str = "meu_evento"
    dados_importantes: str
```

---

## ğŸ› ï¸ Sistema de Componentes

### ğŸ¯ **Componente BÃ¡sico**

```python
from ecsaai import Component, BaseEvent
from typing import List

class MeuComponent(Component):
    name: str = "MeuComponent"

    # Quais eventos recebe e emite (type-safe!)
    receives = [InputEvent]      # Recebe InputEvent
    emits = [OutputEvent]       # Emite OutputEvent

    async def handle_event(self, event: BaseEvent) -> List[BaseEvent]:
        # LÃ³gica do seu componente
        if isinstance(event, InputEvent):
            processado = event.content.upper()

            return [OutputEvent(
                sender=self.name,
                target=f"{self.agent_id}:OutputComponent",
                content=processado,
                session_id=event.session_id
            )]

        return []
```

### ğŸ§© **Componentes DisponÃ­veis**

#### **LLMComponent** - Para uso direto de LLMs

```python
from ecsaai.Components.LLMComponent import LLMComponent, create_fast_llm_config

# Adicionar ao agent
await agent.add_component(
    LLMComponent,
    config=create_fast_llm_config()  # Ou configuraÃ§Ãµes customizadas
)
```

#### **ContextComponent** - Gerenciamento de contexto conversacional

```python
from ecsaai.Components.ContextComponent import ContextComponent, create_chatbot_config

# Com persistÃªncia SQLite
await agent.add_component(
    ContextComponent,
    config=create_chatbot_config()
)

# Ou banco customizado
config = ContextConfig(connection_string="postgresql://...")
```

#### **OutputComponent** - Tratamento de saÃ­das

```python
from ecsaai.Components.OutputComponent import OutputComponent

await agent.add_component(
    OutputComponent,
    config=OutputConfig(output_format="json")
)
```

### ğŸ”— **ComunicaÃ§Ã£o Inter-Componente**

```python
class ComponentComunica(Component):
    async def handle_event(self, event):
        # Chamar outro componente diretamente
        context_response = await self.call_component(
            "ContextComponent",
            GetContextRequest(session_id=event.session_id)
        )

        # Processar resposta...
        return [LLMResponseEvent(content=processada)]
```

---

## ğŸ¯ Middleware System

### ğŸ›¡ï¸ **O que sÃ£o Middlewares?**

Middlewares sÃ£o **interceptadores** que processam eventos automaticamente antes/depois dos componentes:

```
Evento Entrada â†’ Middleware 1 â†’ Middleware 2 â†’ Component â†’ Middleware 2 â†’ Middleware 1 â†’ Evento SaÃ­da
```

### ğŸ”§ **Middlewares Built-in**

#### **LoggingMiddleware** - Log automÃ¡tico

```python
from ecsaai.middleware import LoggingMiddleware

component.middlewares = [LoggingMiddleware()]
# Loga todos os eventos automaticamente
```

#### **RateLimitMiddleware** - Controle de taxa

```python
from ecsaai.middleware import RateLimitMiddleware

component.middlewares = [
    RateLimitMiddleware(requests_per_minute=60)
]
```

#### **MetricsMiddleware** - MÃ©tricas e observabilidade

```python
from ecsaai.middleware import MetricsMiddleware

component.middlewares = [MetricsMiddleware()]
# MÃ©tricas disponÃ­veis em component.metrics
```

#### **CircuitBreakerMiddleware** - TolerÃ¢ncia a falhas

```python
from ecsaai.middleware import CircuitBreakerMiddleware

component.middlewares = [
    CircuitBreakerMiddleware(
        failure_threshold=5,     # ApÃ³s 5 falhas, abre o circuito
        recovery_timeout=60      # Reabre apÃ³s 60 segundos
    )
]
```

#### **ValidationMiddleware** - ValidaÃ§Ã£o automÃ¡tica

```python
from ecsaai.middleware import ValidationMiddleware

component.middlewares = [ValidationMiddleware()]
# Valida automaticamente contratos de entrada/saÃ­da
```

### ğŸ› ï¸ **Criando Middleware Customizado**

```python
from ecsaai import ComponentMiddleware

class MeuMiddleware(ComponentMiddleware):
    async def before_handle(self, event: BaseEvent) -> Optional[BaseEvent]:
        # Antes do componente processar
        print(f"ğŸ“¥ Processando evento: {event.type}")

        # Modificar evento se necessÃ¡rio
        if hasattr(event, 'content'):
            event.content = event.content.strip()

        return event  # Retornar None para CANCELAR o evento

    async def after_handle(self, event: BaseEvent, result: List[BaseEvent]) -> List[BaseEvent]:
        # Depois do componente processar
        print(f"ğŸ“¤ Gerados {len(result)} eventos")

        # Modificar resultados se necessÃ¡rio
        for r in result:
            r.meta['processed_by'] = 'MeuMiddleware'

        return result
```

---

## ğŸ­ Exemplos PrÃ¡ticos

### ğŸ’¬ **Chatbot Simples**

```python
import asyncio
from ecsaai import Agent
from ecsaai.Components.ContextComponent import ContextComponent, create_chatbot_config
from ecsaai.Components.LLMComponent import LLMComponent, create_fast_llm_config
from ecsaai.Components.OutputComponent import OutputComponent
from ecsaai.Events import InputEvent

async def criar_chatbot():
    # 1. Configurar agent
    agent = Agent("chatbot_simples")

    # 2. Adicionar componentes
    await agent.add_component(ContextComponent, config=create_chatbot_config())
    await agent.add_component(LLMComponent, config=create_fast_llm_config())
    await agent.add_component(OutputComponent)

    # 3. Inicializar
    await agent.init_all()

    return agent

async def conversar_com_chatbot():
    agent = await criar_chatbot()

    # Conversa
    mensagens = [
        "OlÃ¡, como vocÃª estÃ¡?",
        "Me conte uma curiosidade sobre IA",
        "Obrigado pela conversa!"
    ]

    for msg in mensagens:
        print(f"ğŸ‘¤ VocÃª: {msg}")

        # Envia input
        evento = InputEvent(
            sender="user",
            target="chatbot_simples:ContextComponent",
            content=msg,
            session_id="conversa_1"
        )

        # Processa pipeline completa
        resultados = await agent.send_event(evento)

        print(f"ğŸ¤– Bot: {len(resultados)} eventos processados")

    await agent.shutdown_all()

# Executar
asyncio.run(conversar_com_chatbot())
```

### ğŸ¤– **Agent com Ferramentas**

```python
from ecsaai import Component
from ecsaai.Events import InputEvent, LLMResponseEvent
from typing import List

class AgentComFerramentas(Component):
    name = "AgentFerramentas"

    receives = [InputEvent]
    emits = [LLMResponseEvent]

    def __init__(self, config=None):
        super().__init__(config)
        # Ferramentas disponÃ­veis
        self.tools = {
            'calcular': self._tool_calcular,
            'busca_web': self._tool_busca_web
        }

    async def handle_event(self, event: InputEvent) -> List[LLMResponseEvent]:
        # AnÃ¡lise bÃ¡sica da intenÃ§Ã£o
        prompt = f"""
        Analise esta mensagem do usuÃ¡rio e determine se precisa de uma ferramenta:

        Mensagem: {event.content}

        Ferramentas disponÃ­veis:
        - calcular: para operaÃ§Ãµes matemÃ¡ticas
        - busca_web: para procurar informaÃ§Ãµes na web

        Responda apenas com o nome da ferramenta ou "nenhuma".
        """

        # Usa LLMFactory para anÃ¡lise
        llm = await self._get_llm()
        analise = await LLMFactory.run(llm, prompt)

        tool_name = analise.content.strip().lower()

        if tool_name in self.tools:
            # Executar ferramenta
            resultado = await self.tools[tool_name](event.content)
            resposta = f"Resultado da ferramenta '{tool_name}': {resultado}"
        else:
            resposta = "Posso ajudar com cÃ¡lculos ou busca na web. O que vocÃª precisa?"

        return [LLMResponseEvent(
            sender=self.name,
            target=f"{self.agent_id}:OutputComponent",
            response=resposta,
            session_id=event.session_id
        )]

    async def _get_llm(self):
        from ecsaai import LLMFactory
        return LLMFactory.create_llm(llm_id="openai:gpt-4o-mini")

    async def _tool_calcular(self, query: str) -> str:
        # ImplementaÃ§Ã£o simples
        try:
            # Extrair expressÃ£o matemÃ¡tica
            import re
            match = re.search(r'(\d[^\w]*\d)', query)
            if match:
                # Usar eval seguro (apenas para demo!)
                result = eval(match.group(1))
                return str(result)
        except:
            return "NÃ£o consegui calcular isso."
        return "NÃ£o encontrei nada para calcular."

    async def _tool_busca_web(self, query: str) -> str:
        # ImplementaÃ§Ã£o mockada
        return f"Resultados mockados para busca: '{query}'"
```

### ğŸ—„ï¸ **Sistema com PersistÃªncia AvanÃ§ada**

```python
from ecsaai.Components.ContextComponent import ContextComponent, ContextConfig

# ConfiguraÃ§Ã£o avanÃ§ada
config = ContextConfig(
    connection_string="postgresql://user:pass@localhost:5432/chatdb",
    initial_system_prompt="""
    VocÃª Ã© um assistente especializado em desenvolvimento de software.
    Sempre forneÃ§a cÃ³digo bem documentado e explique os conceitos.
    """,
    max_history_messages=100,      # Lembra mais mensagens
    auto_cleanup_days=90          # Limpa mensagens antigas
)

component = ContextComponent(config=config)
```

---

## ğŸ”§ Receitas AvanÃ§adas

### ğŸ”„ **Fallback AutomÃ¡tico Entre Provedores**

```python
class FallbackLLMComponent(Component):
    """Componente que testa mÃºltiplos provedores automaticamente"""

    async def get_working_llm(self):
        from ecsaai import LLMFactory

        # Ordem de tentativa
        providers_to_try = [
            "openai:gpt-4o-mini",      # Barato via OpenRouter
            "google:gemini-2.5-flash", # Gratuito atÃ© limite
            "anthropic:claude-3-haiku", # Mais inteligente
            "openai:gpt-3.5-turbo",    # Sempre funciona
        ]

        for provider in providers_to_try:
            try:
                llm = LLMFactory.create_llm(llm_id=provider)
                # Testa conexÃ£o
                test = await LLMFactory.run(llm, "OK", config={'max_tokens': 10})
                if test.content.strip():
                    return llm
            except Exception as e:
                print(f"âŒ {provider} falhou: {str(e)[:50]}")
                continue

        raise RuntimeError("Nenhum provedor LLM funcionando!")
```

### ğŸ“Š **Dashboard de MÃ©tricas**

```python
async def mostrar_metricas_agente(agent):
    print("ğŸ“Š MÃ‰TRICAS DO AGENTE")
    print("=" * 40)

    for name, component in agent.components.items():
        if hasattr(component, 'metrics'):
            m = component.metrics
            print(f"\n{name}:")
            print(f"  ğŸ“¥ Eventos recebidos: {m.events_received}")
            print(f"  ğŸ“¤ Eventos emitidos: {m.events_emitted}")
            print(f"  âŒ Erros: {m.errors}")
            print(f"  â±ï¸ LatÃªncia mÃ©dia: {m.avg_latency_ms:.2f}ms")
            if m.last_error:
                print(f"  ğŸš¨ Ãšltimo erro: {m.last_error[:50]}...")
```

### ğŸ›ï¸ **ConfiguraÃ§Ã£o por Ambiente**

```python
import os
from ecsaai.Components.LLMComponent import LLMConfig

def get_llm_config_por_ambiente():
    env = os.getenv('ENVIRONMENT', 'development')

    configs = {
        'development': LLMConfig(  # Mais barato para dev
            llm_id="openai:gpt-4o-mini",
            temperature=0.7,
            max_tokens=500
        ),
        'production': LLMConfig(   # Melhor qualidade para prod
            llm_id="openai:gpt-4",
            temperature=0.1,
            max_tokens=2000
        ),
        'testing': LLMConfig(      # Mock para testes
            llm_id="google:gemini-2.5-flash",
            temperature=0.0      # DeterminÃ­stico
        )
    }

    return configs.get(env, configs['development'])
```

### ğŸ”„ **Hot Reload de ConfiguraÃ§Ã£o**

```python
from ecsaai.middleware import ComponentMiddleware

class HotReloadMiddleware(ComponentMiddleware):
    """Middleware que recarrega configuraÃ§Ã£o automaticamente"""

    def __init__(self, config_file: str):
        self.config_file = config_file
        self.last_modified = 0

    async def before_handle(self, event):
        # Verifica se arquivo mudou
        import os
        current_mtime = os.path.getmtime(self.config_file)

        if current_mtime > self.last_modified:
            print("ğŸ”„ ConfiguraÃ§Ã£o mudada, recarregando...")

            # Recarregar config (component.config.reload_from_file() etc)
            self.last_modified = current_mtime

        return event
```

---

## ğŸ› Troubleshooting

### âŒ **Erro: API key nÃ£o encontrada**

```python
# Verificar variÃ¡veis de ambiente
import os
print("GOOGLE_API_KEY:", bool(os.getenv('GOOGLE_API_KEY')))
print("OPENROUTER_API_KEY:", bool(os.getenv('OPENROUTER_API_KEY')))

# Verificar arquivo .env existe
import os.path
print(".env existe:", os.path.exists('.env'))
```

### âŒ **Erro: Provider nÃ£o suportado**

```python
# Ver provedores disponÃ­veis
from ecsaai import LLMFactory
print("DisponÃ­veis:", LLMFactory.list_available_models())
```

### âŒ **Erro: Timeout**

```python
# Aumentar timeout
llm = LLMFactory.create_llm(
    llm_id="google:gemini-2.5-flash",
    config={'timeout': 120}  # 2 minutos
)
```

### âŒ **Erro: Rate limit**

```python
# Adicionar middleware de rate limit
from ecsaai.middleware import RateLimitMiddleware

component.middlewares = [
    RateLimitMiddleware(requests_per_minute=30)  # 30 por minuto
]
```

### âŒ **Erro: Component jÃ¡ existe**

```python
# Verificar nomes Ãºnicos
async def adicionar_com_nome_unico(agent, component_class, config):
    nome_unico = f"{component_class.__name__}_{id(config)}"
    component_class.name = nome_unico
    await agent.add_component(component_class, config)
```

---

## ğŸ¯ PrÃ³ximos Passos

1. **ğŸ“š Leia a documentaÃ§Ã£o completa** em [ecsaai-docs.com](https://ecsaai-docs.com)
2. **ğŸ® Experimente os tutoriais** em `/tutorial_comprehensive.md`
3. **ğŸ› ï¸ Crie componentes customizados** seguindo o padrÃ£o
4. **ğŸ“Š Monitore mÃ©tricas** com middlewares
5. **ğŸš€ FaÃ§a deploy** usando os presets de produÃ§Ã£o

---

**ğŸ‰ ParabÃ©ns!** VocÃª agora domina o ECSAI Framework. Com ele, vocÃª pode construir aplicaÃ§Ãµes de IA robustas, escalÃ¡veis e fÃ¡ceis de manter. Boa sorte com seus projetos! ğŸš€
